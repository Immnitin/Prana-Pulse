{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTb-9TFFqprC"
      },
      "source": [
        "Importing the Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3q9U3S_whh3-"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sklearn'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
          ]
        }
      ],
      "source": [
        "import sklearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egMd5zeurTMR"
      },
      "source": [
        "Data Collection and Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0q-3-LkQrREV"
      },
      "outputs": [],
      "source": [
        "# loading the csv data to a Pandas DataFrame\n",
        "heart_data = pd.read_csv('heart_disease_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "M8dQxSTqriWD",
        "outputId": "33252faa-5554-4fcc-f003-718b899cc400"
      },
      "outputs": [],
      "source": [
        "# print first 5 rows of the dataset\n",
        "heart_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "Fx_aCZDgrqdR",
        "outputId": "c8a5e037-23ed-4913-caf2-2156f7e1b3a6"
      },
      "outputs": [],
      "source": [
        "# print last 5 rows of the dataset\n",
        "heart_data.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nX1tIzbrz0u",
        "outputId": "76a5c2ef-ec03-4531-f4ab-26742afd7310"
      },
      "outputs": [],
      "source": [
        "# number of rows and columns in the dataset\n",
        "heart_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_xTcw1Sr6aJ",
        "outputId": "1c0f53ae-a949-4dd6-cd11-39142743b81f"
      },
      "outputs": [],
      "source": [
        "# getting some info about the data\n",
        "heart_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "GjHtW31rsGlb",
        "outputId": "3121ca90-ba75-47e8-fe56-3dc885eb400e"
      },
      "outputs": [],
      "source": [
        "# checking for missing values\n",
        "heart_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "OHmcP7DJsSEP",
        "outputId": "2001a980-8594-4627-9089-7188e06e3b34"
      },
      "outputs": [],
      "source": [
        "# statistical measures about the data\n",
        "heart_data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "4InaOSIUsfWP",
        "outputId": "6417b0d2-c5fc-4c4e-b00a-2e9503a1718d"
      },
      "outputs": [],
      "source": [
        "# checking the distribution of Target Variable\n",
        "heart_data['target'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSOBu4qDtJy5"
      },
      "source": [
        "1 --> Defective Heart\n",
        "\n",
        "0 --> Healthy Heart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW8i4igjtPRC"
      },
      "source": [
        "Splitting the Features and Target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6yfbswrs7m3"
      },
      "outputs": [],
      "source": [
        "X = heart_data.drop(columns='target', axis=1)\n",
        "Y = heart_data['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJoCp4ZKtpZy",
        "outputId": "a6f3d6b9-392f-4e53-f8b6-e23de191f874"
      },
      "outputs": [],
      "source": [
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7pfbxSGhLfe",
        "outputId": "a2527230-61a6-446b-d72e-a8d7de4547e1"
      },
      "outputs": [],
      "source": [
        "X.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nukuj-YItq1w",
        "outputId": "4df0fbbe-f68f-408b-dda6-8cda358afce3"
      },
      "outputs": [],
      "source": [
        "print(Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odSwxSsZAUmY"
      },
      "source": [
        "### **Data Standardization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ipDPcq7Ad2I"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "oIFpZpB9AeCg",
        "outputId": "192c8d90-f3f8-40e3-d843-d58563574da0"
      },
      "outputs": [],
      "source": [
        "scaler.fit(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScWv0HMYAla5"
      },
      "outputs": [],
      "source": [
        "standardized_data = scaler.transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qls4LKUjAoBj",
        "outputId": "2746b975-1b4a-459e-f902-0217eb246bec"
      },
      "outputs": [],
      "source": [
        "print(standardized_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSXAIK24ApvJ"
      },
      "outputs": [],
      "source": [
        "X = standardized_data\n",
        "Y = heart_data['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPPbFDd0AtHw",
        "outputId": "94be86d9-08d5-4e6e-cad8-afde83175ace"
      },
      "outputs": [],
      "source": [
        "print(X)\n",
        "print(Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EcjSE3Et18n"
      },
      "source": [
        "### **Splitting the Data into Training data & Test Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-UUfRUxtuga"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7PrjC6zuf6X",
        "outputId": "a6e6dc60-99c4-4e29-a28d-09086526a583"
      },
      "outputs": [],
      "source": [
        "print(X.shape, X_train.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beSkZmpVuvn9"
      },
      "source": [
        "Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi2NOWZjuxzw"
      },
      "source": [
        "Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-Md74FYuqNL"
      },
      "outputs": [],
      "source": [
        "model = LogisticRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "kCdHYxGUu7XD",
        "outputId": "e5965d42-5314-4c1e-e512-f9c115376ee4"
      },
      "outputs": [],
      "source": [
        "# training the LogisticRegression model with Training data\n",
        "model.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYIw8Gi9vXfU"
      },
      "source": [
        "*Model* Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmxAekfZvZa9"
      },
      "source": [
        "Accuracy Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g19JaUTMvPKy"
      },
      "outputs": [],
      "source": [
        "# accuracy on training data\n",
        "X_train_prediction = model.predict(X_train)\n",
        "training_data_accuracy = accuracy_score(X_train_prediction, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQBZvBh8v7R_",
        "outputId": "55b7b3ff-edf8-4bb0-f630-83a133ec2348"
      },
      "outputs": [],
      "source": [
        "print('Accuracy on Training data : ', training_data_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDONDJdlwBIO"
      },
      "outputs": [],
      "source": [
        "# accuracy on test data\n",
        "X_test_prediction = model.predict(X_test)\n",
        "test_data_accuracy = accuracy_score(X_test_prediction, Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MBS-OqdwYpf",
        "outputId": "0fb4cac7-71a0-4fe5-e5fc-4038ce148132"
      },
      "outputs": [],
      "source": [
        "print('Accuracy on Test data : ', test_data_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qG-Gyw9Zjdi",
        "outputId": "25381ac2-2870-4c68-a529-04984ceb4065"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"--- Training and Evaluating Random Forest ---\")\n",
        "\n",
        "# Initialize the Random Forest model\n",
        "# n_estimators=100 means it will build 100 decision trees\n",
        "# random_state=42 ensures you get the same result every time you run it\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model on the same scaled training data\n",
        "rf_model.fit(X_train, Y_train)\n",
        "\n",
        "# Make predictions on the scaled test data\n",
        "Y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the new model\n",
        "rf_accuracy = accuracy_score(Y_test, Y_pred_rf)\n",
        "\n",
        "# Print the accuracy score for the Random Forest model\n",
        "print(f\"Random Forest Test Accuracy: {rf_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIruVh3Qwq0e"
      },
      "source": [
        "### **Building a Predictive System**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ercruC9wb4C",
        "outputId": "7542dbc7-bfb6-4667-e9a0-72158b519530"
      },
      "outputs": [],
      "source": [
        "input_data = (57,\t0\t,0\t,120\t,354\t,0\t,1\t,163\t,1\t,0.6\t,2\t,0\t,2)\n",
        "\n",
        "# change the input data to a numpy array\n",
        "input_data_as_numpy_array= np.asarray(input_data)\n",
        "print(input_data_as_numpy_array.dtype)\n",
        "\n",
        "# reshape the numpy array as we are predicting for only on instance\n",
        "input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)\n",
        "print(input_data_reshaped.dtype)\n",
        "\n",
        "prediction = model.predict(input_data_reshaped)\n",
        "print(prediction)\n",
        "\n",
        "if (prediction[0]== 0):\n",
        "  print('The Person does not have a Heart Disease')\n",
        "else:\n",
        "  print('The Person has Heart Disease')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eior1C6ibMiw"
      },
      "source": [
        "### **Saving the trained model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lgAbtLrJkHS"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pedkzgZKbQLQ"
      },
      "outputs": [],
      "source": [
        "filename = 'heart_disease_model.sav'\n",
        "pickle.dump(model, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdN9lpGibQR5"
      },
      "outputs": [],
      "source": [
        "# loading the saved model\n",
        "loaded_model = pickle.load(open('heart_disease_model.sav', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jY7hbuJ_bQaf",
        "outputId": "d5845ef8-c82e-4400-ec92-79003c1a97d8"
      },
      "outputs": [],
      "source": [
        "# The X variable is now a NumPy array after standardization, so it doesn't have the .columns attribute.\n",
        "# We can use the columns from the original heart_data DataFrame (excluding the 'target' column)\n",
        "# to label the features in the standardized data.\n",
        "\n",
        "for i, column in enumerate(heart_data.drop(columns='target', axis=1).columns):\n",
        "  print(f\"Column {i}: {column}\")\n",
        "\n",
        "# Alternatively, if you just want to iterate through indices:\n",
        "# for i in range(X.shape[1]):\n",
        "#     print(f\"Column index: {i}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlvin55ubQhR",
        "outputId": "85d240fa-98e8-4af8-dea0-4dfc46fecc72"
      },
      "outputs": [],
      "source": [
        "print(sklearn.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        },
        "id": "uTgE6S1mV5Ts",
        "outputId": "3526f6b8-d296-423d-b3f0-2672904662c1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Ensure you have run the OneHotEncoder step from my previous guidance first\n",
        "# The feature names should match the columns after one-hot encoding\n",
        "# feature_names = X.columns # This was causing the error\n",
        "# encoded_feature_names = list(X_train.columns) # This was also causing the error\n",
        "\n",
        "# Get the feature names from the original DataFrame before standardization\n",
        "original_feature_names = heart_data.drop(columns='target', axis=1).columns\n",
        "encoded_feature_names = list(original_feature_names)\n",
        "\n",
        "\n",
        "# Get the coefficients from the trained logistic regression model\n",
        "coefficients = model.coef_[0]\n",
        "\n",
        "# Create a DataFrame to view the feature importances\n",
        "feature_importance = pd.DataFrame({'Feature': encoded_feature_names, 'Importance': coefficients})\n",
        "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot the feature importances\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
        "plt.title('Feature Importance from Logistic Regression')\n",
        "plt.show()\n",
        "\n",
        "print(feature_importance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqokrDBZcnge",
        "outputId": "92cd33ce-a9da-41ee-e598-22d33900e5cc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# Note: In your actual notebook, the 'scaler' and 'model' objects\n",
        "# would already be trained and ready. Here, we're just showing the logic.\n",
        "\n",
        "# --- Step 1: The Recommendation Function ---\n",
        "# This is the function we developed earlier.\n",
        "def generate_recommendations(prediction, data_input):\n",
        "    \"\"\"\n",
        "    Generates personalized health recommendations based on model prediction.\n",
        "\n",
        "    Args:\n",
        "        prediction (int): The model's prediction (0 for No Disease, 1 for Disease).\n",
        "        data_input (dict): A dictionary with feature names and the user's values.\n",
        "    \"\"\"\n",
        "    print(\"--- Your Health Assessment ---\")\n",
        "    if prediction == 0:\n",
        "        print(\"✅ Prediction: Low risk of heart disease.\")\n",
        "        print(\"\\nRecommendations: Great job! Continue to maintain your healthy lifestyle.\")\n",
        "        print(\"- Continue regular check-ups with your doctor.\")\n",
        "        print(\"- Maintain a balanced diet and regular exercise.\")\n",
        "        return\n",
        "\n",
        "    # If the prediction is 1 (high risk), provide specific advice\n",
        "    print(\"❗️ Prediction: High risk of heart disease.\")\n",
        "    print(\"\\nBased on our model, here are some key areas to discuss with your doctor:\")\n",
        "\n",
        "    # Chest Pain is a major factor\n",
        "    if data_input.get('cp', 0) > 0:\n",
        "        print(\"- Your reported chest pain type is a significant risk factor. It is crucial to discuss this with a cardiologist.\")\n",
        "\n",
        "    # Exercise-induced angina\n",
        "    if data_input.get('exang', 0) == 1:\n",
        "        print(\"- Exercise-induced angina ('exang') was noted. This should be investigated further with a stress test.\")\n",
        "\n",
        "    # Cholesterol and Blood Pressure\n",
        "    if data_input.get('chol', 0) > 200 or data_input.get('trestbps', 0) > 130:\n",
        "        print(\"- Focus on managing your cholesterol and blood pressure through diet, exercise, and possibly medication as advised by your doctor.\")\n",
        "\n",
        "    print(\"\\nDisclaimer: This is an AI-generated assessment and not a substitute for professional medical advice.\")\n",
        "\n",
        "\n",
        "# --- Step 2: Simulate the End-to-End Workflow ---\n",
        "# Imagine this is your main script or dashboard backend.\n",
        "\n",
        "# Assume 'model' and 'scaler' are your already trained objects from the notebook.\n",
        "# For this example to run standalone, we'll create dummy objects.\n",
        "# In your real code, you would load your saved model and scaler.\n",
        "if 'model' not in globals() or 'scaler' not in globals():\n",
        "    print(\"Creating dummy model and scaler for demonstration purposes.\")\n",
        "    # This is just to make the example runnable. Use your actual trained objects.\n",
        "    scaler = StandardScaler().fit(X_train) # Using X_train from your notebook\n",
        "    model = LogisticRegression().fit(scaler.transform(X_train), Y_train)\n",
        "\n",
        "\n",
        "# 1. Get new user input (imagine this came from a web form)\n",
        "new_user_data = {\n",
        "    'age': 52, 'sex': 1, 'cp': 0, 'trestbps': 125, 'chol': 212,\n",
        "    'fbs': 0, 'restecg': 1, 'thalach': 168, 'exang': 0, 'oldpeak': 1.0,\n",
        "    'slope': 2, 'ca': 2, 'thal': 3\n",
        "}\n",
        "print(f\"Analyzing data for a {new_user_data['age']}-year-old...\")\n",
        "\n",
        "\n",
        "# 2. Format the input for the model\n",
        "# Create a DataFrame, ensure column order is the same as training, then scale it.\n",
        "input_df = pd.DataFrame([new_user_data])\n",
        "input_scaled = scaler.transform(input_df)\n",
        "\n",
        "\n",
        "# 3. Make a prediction\n",
        "user_prediction = model.predict(input_scaled)[0] # Get the single prediction (0 or 1)\n",
        "\n",
        "\n",
        "# 4. Generate personalized recommendations\n",
        "generate_recommendations(user_prediction, new_user_data)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
